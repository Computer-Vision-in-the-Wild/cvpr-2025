<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!--Open Graph Related Stuff-->
    <meta property="og:title" content="Workshop on Computer Vision in the Wild 2025" />
    <meta property="og:url" content="https://computer-vision-in-the-wild.github.io/cvpr-2025/" />
    <meta property="og:description" content="June 11 at CVPR 2025" />
    <meta property="og:description" content="CVPR 2025" />
    <meta property="og:site_name" content="Computer Vision in the Wild 2025" />
    <meta property="og:image" content="https://computer-vision-in-the-wild.github.io/cvpr-2025/static/cvpr2025/img/head_img/cvpr_2025_header.jpeg" />
    <meta property="og:image:url" content="https://computer-vision-in-the-wild.github.io/cvpr-2025/static/cvpr2025/img/head_img/cvpr_2025_header.jpeg" />
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Workshop on Computer Vision in the Wild 2025">
    <meta name="twitter:description" content="June 11 at CVPR 2025">
    <meta name="twitter:description" content="CVPR 2025">
    <meta name="twitter:image" content="https://computer-vision-in-the-wild.github.io/cvpr-2025/static/cvpr2025/img/head_img/cvpr_2025_header.jpeg">
    <title>Computer Vision in the Wild</title>
    <link rel="stylesheet" href="./static/css/foundation.css">
    <link rel="stylesheet" href="./static/css/main.css">
    <script src="./static/js/vendor/jquery.js"></script>
    <script src="./static/js/jquery-2.1.3.min.js"></script>

    <script type="text/javascript" src="./static/js/jquery.countdown.min.js"></script>
    <script type="text/javascript" src="./static/js/moment.min.js"></script>
    <script type="text/javascript" src="./static/js/moment-timezone-with-data.min.js"></script>

    <script type="text/javascript" src="./static/js/main-vqa.js"></script>
    <script type="text/javascript" src="./static/js/main-gqa.js"></script>
    <script type="text/javascript" src="./static/js/main-visdial.js"></script>
    <script type="text/javascript" src="./static/js/main-textvqa.js"></script>
    <script type="text/javascript" src="./static/js/main-textcaps.js"></script>
    <script type="text/javascript" src="./static/js/main-vizwiz.js"></script>

</head>
<style type="text/css">
.schedule table {
    -webkit-border-radius: 5px;
    -moz-border-radius: 5px;
    border-radius: 5px;
}

.schedule tr:hover {
    background-color: #D3D3D3;
}

.schedule img {
    max-height: 95px;
    max-width: 140px;
    margin-right: 2px;
    margin-top: 4px;
    margin-bottom: 4px;
    border-radius: 50%;
}

.abstract-content {
    display: none;
    padding: 5px;
}

.disclaimer {
    padding-left: 25px;
    text-align: left;
    font-size: 14px;
    line-height: 16px;
}
.disclaimer b {
    font-weight: bold !important;
}
</style>

<body class="off-canvas hide-extras" style="min-width:1300px; min-height:750px;">
    <header>
        <div class="row">
            <a href="https://computer-vision-in-the-wild.github.io/cvpr-2025"><img style="height: 160px; position:absolute; top:420px; right:26px;" src="./static/cvpr2025/img/head_img/cvpr_2025_logo.jpg" alt="logo" /></a>
            <!-- <h1><img style="height: 200px;" src="./static/eccv2022/img/cvinthewild_logo.jpg" alt="logo" /><br></h1> -->
            <br>
        </div>
    </header>
    <div class="contain-to-grid">
        <!-- <nav class="top-bar" data-topbar> -->
            <!-- <section class="top-bar-section"> -->
                <!-- Right Nav Section -->
                <!-- <ul class="right"> -->
                    <!-- <li><a href="index.html">Home</a></li> -->
                    <!-- <li><a href="people.html">People</a></li> -->
                    <!-- <li><a href="code.html">Code</a></li> -->
                    <!-- <li><a href="http://vqa.cloudcv.org/" onClick="ga('send', 'event', { eventCategory: 'Outgoing Link', eventAction: 'Demo', eventLabel: 'Demo'});">Demo</a></li> -->
                    <!-- <li class="has-dropdown"><a href="download.html">Download</a> -->
                        <!-- <ul class="dropdown"> -->
                            <!-- <li><a href="download.html">VQA v2</a></li> -->
                            <!-- <li><a href="vqa_v1_download.html">VQA v1</a></li> -->
                        <!-- </ul> -->
                    <!-- </li> -->
                    <!-- <li><a href="evaluation.html">Evaluation</a></li>
                    <li class="has-dropdown"><a href="challenge.html">Challenge</a>
                        <ul class="dropdown">
                            <li><a href="challenge.html">2021</a></li>
                            <li><a href="challenge_2020.html">2020</a></li>
                            <li><a href="challenge_2019.html">2019</a></li>
                            <li><a href="challenge_2018.html">2018</a></li>
                            <li><a href="challenge_2017.html">2017</a></li>
                            <li><a href="challenge_2016.html">2016</a></li>
                        </ul>
                    </li> -->
                    <!-- <li class="has-dropdown"><a href="http://visualqa.org/vqa_v2_teaser.html">Browse</a>
                        <ul class="dropdown">
                            <li><a href="http://visualqa.org/vqa_v2_teaser.html">VQA v2</a></li>
                            <li><a href="https://vqabrowser.cloudcv.org/">VQA v1</a></li>

                        </ul>
                    </li> -->
                    <!-- <li><a href="http://visualqa.org/visualize/">Visualize</a></li> -->
                    <!-- <li class="active has-dropdown"><a href="workshop.html">Workshop</a>
                        <ul class="dropdown"> -->
                            <!-- <li><a href="workshop.html"></a></li> -->
                            <!-- <li><a href="workshop_2020.html">2020</a></li>
                            <li><a href="workshop_2019.html">2019</a></li>
                            <li><a href="workshop_2018.html">2018</a></li>
                            <li><a href="workshop_2017.html">2017</a></li>
                            <li><a href="workshop_2016.html">2016</a></li> -->
                        <!-- </ul> -->
                    <!-- </li> -->
                    <!-- <li><a href="sponsors.html">Sponsors</a></li> -->
                    <!-- <li><a href="terms.html">Terms</a></li> -->
                    <!-- <li><a href="external.html">External</a></li> -->
                <!-- </ul> -->
            <!-- </section> -->
        <!-- </nav> -->
    </div>
    <section role="main" style="padding: 1em;">
        <div class="row">
            <p style="font-size:50px; color:black; font-weight: 50" align=center>
                The 4th Workshop on Computer Vision in the Wild <br> 
                <span style="font-size:30px; color:gray; font-weight: 50" align=center> <b>Theme</b>: Building Multimodal AI with both Verbal, Spatial and Temporal Intelligence</span>
                <br>
                <span style="font-size:25px; color:gray; font-weight: 50" align=center>Date: June 11 | Location: Music City Center, Nashville TN<br> </span>

<!--                  <span style="font-size:20px; color:gray; font-weight: 30" align=center>9:00am-6:00pm Israeli Time  ||  11:00pm (October 22)-8:00am Pacific Time  ||  2:00pm-11:00pm Beijing Time</span>   -->             
                <!-- <br> -->
                <!-- <br> -->
                <!-- <span style="font-size:20px; color:black; font-weight: 400" align=center>Zoom and Gatherly links on ECCV 2022 website: <a target="_blank" href="https://www.eventscribe.net/2021/2021CVPR/agenda.asp?startdate=6/19/2021&enddate=6/19/2021&BCFO=M&pfp=Workshops&mode=&tn=&cpftwo=&custwo=&pta=/">Link</a> <br> Navigate to: Workshops -> Sat, October 23 -> Search for "Computer Vision in the Wild" workshop entry</b></span> -->

            </p>




        <div class="row">
            <!-- <h1 style="font-size:30px; color:grey; font-weight: 200">Overview</h1> -->
            <h2>Overview</h2>
            <p>As artificial intelligence continues to evolve, the intersection of vision and language models is becoming increasingly crucial for real-world applications. The <strong>4th Workshop on Computer Vision in the Wild (CVinW)</strong> at <strong><a href="https://cvpr.thecvf.com/">CVPR 2025</a></strong> aims to foster discussions and innovations that push the boundaries of computer vision systems in unconstrained environments. Building on the success of our previous workshops: <a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/"> CVPR 2024 CVinW Workshop</a>, <a href="https://computer-vision-in-the-wild.github.io/cvpr-2023/"> CVPR 2023 CVinW Workshop</a> and
                <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/"> ECCV 2022 CVinW Workshop</a>, this edition will focus on the next generation of <strong>large multimodal models (LMMs)</strong> and <strong>vision-language-action (VLA) systems</strong>, with an emphasis on <strong>temporal reasoning, video understanding, and physical interaction</strong>.</p>
            <br>
            <br>
            <img src="./static/cvpr2025/img/misc/cv_roadmap.png" alt="header" style="width:100%; margin-top: 0px; margin-bottom: 0px;"/>
            <p style="font-size:12px; text-align:center;">Image source: <a href="https://arxiv.org/abs/2210.09263">Vision-Language Pre-training: Basics, Recent Advances, and Future Trends</a> and <a href="https://arxiv.org/abs/2309.10020">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a></p>
            <br>
            <p>Over the past years, we have witnessed remarkable advancements in <strong>open-vocabulary</strong> visual comprehension models and <strong>multimodal learning</strong>. Five years ago, vision-language models or multimodal models are mostly built on top of the BERT architecture. Typically, these models contain less than 1B parameters, and trained with a small amount of images. Some representative works are like <a href="https://arxiv.org/abs/1908.02265">ViLBERT<a>, <a href="https://arxiv.org/abs/1909.11740">UNITER</a>, and <a href="https://arxiv.org/abs/1908.03557">VisualBERT</a>, etc. They are mostly used for image-text matching tasks such as visual question answering (VQA) and image captioning. 
                
            <!-- Later on, we have seen the emergence of large-scale vision-language models such as Flamingo, Gemini, and GPT-4V. These models are trained with billions of images and text pairs, and they show strong zero-shot transfer capability across a wide range of visual tasks. -->

            Later on, we have seen the emergence of multimodal vision foundation models, such as <a href="">CLIP</a>, <a href="">ALIGN</a> and <a href="">Florence</a>. It scaled up the multimodal training to billions of images. Despite the model size is still relatively small, it shows strong open-vocabulary and zero-shot recognition capability across a wide range of visual domains. These strong capabilities have been further transferred to fine-grained core vision tasks such as object detection (e.g., <a href="https://arxiv.org/abs/2104.12763">M-DETR</a>, <a href="https://arxiv.org/abs/2104.13921">ViLD</a>, <a href="https://arxiv.org/abs/2112.03857">GLIP</a>, <a href="https://arxiv.org/abs/2112.09106">RegionCLIP</a>, <a href="https://arxiv.org/abs/2303.05499">GroundingDINO</a>, <a href="https://arxiv.org/abs/2205.06230">OWL-ViT</a>, etc), image segmentations (e.g., <a href="https://arxiv.org/abs/2212.11270">X-Decoder</a>, <a href="https://arxiv.org/abs/2304.03284">SegGPT</a>, <a href="https://arxiv.org/abs/2304.06718">SEEM</a>, <a href="https://arxiv.org/abs/2304.02643">SAM</a>, <a href="https://arxiv.org/abs/2308.00692">LISA</a>, etc).
            
            Most recently, we entered the era of large multimodal models. Connecting the multimodal vision models such as CLIP with large language models such as <a href="https://arxiv.org/abs/2204.14198">Flamingo</a>, <a href="https://arxiv.org/abs/2312.11805">Gemini</a>, <a href="https://openai.com/index/gpt-4v-system-card/">GPT-4V</a>, leading to many advanced multimodal capability. Now we can have a multimodal chatbot such as <a href="https://openai.com/index/gpt-4o-system-card/">GPT-4o</a>, <a href="https://arxiv.org/abs/2408.03326">LLaVA-OneVision</a>, <a href="https://arxiv.org/abs/2502.13923">Qwen-2.5-VL</a> and <a href="https://arxiv.org/abs/2503.01743">Phi-4-Multimodal</a>, which can see, talk and reasoning</p>
            
            <p>Despite these successes, current vision models still lack the ability to fully grasp <strong>temporal dynamics, causal reasoning, and embodied interactions</strong>‚Äîkey elements for autonomous agents that can <strong>see, reason, and act</strong>. Some recent works have attempted to address these challenges by building agentic models and VLA models. Our workshop aims to <strong>bringing together leading researchers, practitioners, and industry experts</strong> to discuss these emerging trends, challenges, and solutions in the field.</p>
    
            <h2>Highlights</h2>
            <ul>
                <p><strong>(1) Invited Talks</strong> from leading experts in academia and industry on the latest advancements in multimodal AI.</p>    

                <p><strong>(2) Paper Presentations</strong> showcasing cutting-edge research contributions in computer vision in the wild.</p>
                
                <p><strong>(3) Panel Discussions (Tentative)</strong> exploring the future of vision-language-action models and their impact on robotics, autonomous systems, and real-world AI applications.</p>
                <!-- <li><strong>Two Research Challenges</strong> designed to benchmark and evaluate state-of-the-art models:
                    <ul>
                        <li><strong>Fine-grained Temporal Dynamics Understanding (TemporalBench)</strong></li>
                        <li><strong>Video-based World Modeling Evaluation (MMWorld)</strong></li>
                    </ul>
                </li> -->
            </ul>
    
            <p>We invite researchers, engineers, and enthusiasts to join us in shaping the future of vision systems that go beyond static image recognition to <strong>dynamic, interactive, and real-world AI applications</strong>. Stay tuned for more details on speakers, paper submissions, and challenge participation!</p>
    
            <p>For more information, visit our <strong>official workshop page</strong> and explore our <strong>CVinW reading list</strong>: üìå <a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings" target="_blank">CVinW Readings</a></p>            
            <!-- <h1 style="font-size:30px; color:grey; font-weight: 200">Overview</h1>
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 400; text-align:left">

                A long-standing aspiration in artificial intelligence is to develop general-purpose assistants that can effectively follow users‚Äô (multimodal) instructions to complete a wide range of real-world tasks. Recently, the community has witnessed a growing interest in developing foundation models with emergent abilities of multimodal understanding and generation in open-world tasks. While the recipes of using large language models (LLMs) such as ChatGPT to develop general-purpose assistants for natural language tasks have been proved effective, the recipes of building general-purpose, multimodal assistants for computer vision and vision-language tasks in the wild remain to be explored. Recent works show that learning from large-scale image-text data with human feedback in the loop is a promising approach to building transferable visual models that can effortlessly adapt to a wide range of downstream computer vision (CV) and multimodal (MM) tasks. For example, large multimodal models (LMM) such as Flamingo, GPT-4V, Gemini have demonstrated strong zero-shot transfer capabilities on many vision tasks in the wild. The open-source LMMs have also made significant progress, as demonstrated by OpenFlamingo, <a href="https://minigpt-4.github.io/"> MiniGPT4</a> and <a href="https://llava-vl.github.io/"> LLaVA</a>. These models are trained with visual instruction-following data, where human intents are represented in natural language. On the other hand, interactive vision systems such as <a href="https://github.com/facebookresearch/segment-anything">Segment Anything (SAM)</a> and <a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once">SEEM</a> have also shown impressive segmentation performance on almost anything in the wild, where human intents are represented in visual prompts, such as click, bounding boxes and text. These vision models with language and multimodal interfaces are naturally open-vocabulary and even open-task models, showing superior zero-shot performance in various real-world scenarios.

                We host this ‚ÄúComputer Vision in the Wild (CVinW)‚Äù workshop, aiming to gather academic and industry communities to work on CV problems in real-world scenarios, focusing on the challenge of open-world visual task-level transfer. This CVPR 2025 4th CVinW workshop is a continuation of <a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/"> CVPR 2024 CVinW Workshop</a>, <a href="https://computer-vision-in-the-wild.github.io/cvpr-2023/"> CVPR 2023 CVinW Workshop</a> and
                <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/"> ECCV 2022 CVinW Workshop</a>. For those who are new to this topic, please check out the  <a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/blob/main/README.md">CVinW Reading List</a>.
                
                <br>
                <br> -->
                
                <!-- The development of LMMs is an emerging new filed, with a vast research exploration space in data collection, modeling, evaluation and new application scenarios. There are many new evaluation and benchmarks merged to measure their performance from different aspects. To advocate established benchmarks to measure the progress, this workshop welcome authors different benchmarks to run indepedent challenges and report results. We highlight a few recent LMM evaluation toolkit / benchmarks: -->
                <!-- <ul style="font-size:15px;">

                    
                        <li style="margin-left:30px">
                            <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">  <b> LMMs-Eval: The Evaluation Suite of Large Multimodal Models </b></a> 
                        </li>  
                        <li style="margin-left:30px">
                            <b> <a href="https://huggingface.co/spaces/WildVision/vision-arena">WildVision Arena: Evaluating Vision-Language Models in the Wild with Human Preferences </a></b>
                        </li>  
                        <li style="margin-left:30px">
                            <a href="https://huggingface.co/spaces/mlfoundations/VisIT-Bench-Leaderboard"> VisIT-Bench: Vision-Language Instruction Following </a>
                        </li>

                </ul>          -->

                </p>
            </div>
            <hr>
        <!-- </div> -->





<!-- Invited Speakers -->

 <!-- <div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Keynote Speaker</h1>
    <div class="team" id="people">
        <div class="row" style="display: flex; text-align:center; justify-content: center;">

            <div class="large-1 columns">
                <p></p>
            </div>


            <div class="large-4 columns">
                <a href="https://www.andrewng.org/"><img src="./static/cvpr2024/img/speakers/andrew-ng.png" class="speaker_picture" style="width:200px; height:200px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Andrew Ng
                    <br> Founder of <a href="https://www.deeplearning.ai/">DeepLearning.AI</a>, <a href="https://landing.ai/">Landing AI</a>, General Partner at <a href="https://aifund.ai/">AI Fund</a>, Chairman and Co-Founder of <a href="https://www.coursera.org/">Coursera</a> and an Adjunct Professor at Stanford University. </p>
            </div>

 


            <div class="large-1 columns">
                <p></p>
            </div>


            <div class="large-1 columns">
                <p></p>
            </div>
        </div>

    <hr>
</div> -->


<br>
<br>
  <!-- <div class="row"> -->
        <!-- <h1 style="font-size:30px; color:grey; font-weight: 200">Invited Speakers</h1> -->
    <h2 style="text-align:center;">Invited Speakers</h2>
    <br>
        <div class="team" id="people">
            <div class="row" style="display: flex; text-align:center; justify-content: center;">

                <div class="large-2 columns">
                    <a href="https://cshizhe.github.io/"><img src="./static/cvpr2025/img/speakers/shizhe_chen.jpg"  class="speaker_picture" style="width:180px; height:180px;">
                        <br><br>
                    </a>
                    <p style="font-size:16px; font-weight: 200;">Shizhe Chen
                        <br>Inria Paris
                </div>

                <div class="large-2 columns">
                    <a href="http://boqinggong.info/"><img src="./static/cvpr2025/img/speakers/boqing_gong.jpg"  class="speaker_picture" style="width:180px; height:180px;">
                        <br><br>
                    </a>
                    <p style="font-size:16px; font-weight: 200;">Boqing Gong
                        <br>Boston University</p>
                </div>  

                <div class="large-2 columns">
                    <a href="https://cordeliaschmid.github.io/"><img src="https://cordeliaschmid.github.io/images/profil_cd.jpg" class="speaker_picture" style="width:180px; height:180px;">
                        <br><br>
                    </a>
                    <p style="font-size:16px; font-weight: 200;">Cordelia Schmid
                        <br>Inria | Google</p>
                </div>
    
                <div class="large-2 columns">
                    <a href="https://www.ranjaykrishna.com/index.html"><img src="https://www.ranjaykrishna.com/uploads/3/4/4/6/34464733/headshot_orig.jpeg" class="speaker_picture" style="width:180px; height:180px;">
                        <br><br>
                    </a>
                    <p style="font-size:16px; font-weight: 200;">Ranjay Krishna 
                        <br>University of Washington</p>
                </div>

                <div class="large-1 columns">
                    <p></p>
                </div>
            </div>

            <div class="row" style="display: flex; text-align:center; justify-content: center;">


                <div class="large-2 columns">
                    <a href="https://www.sainingxie.com/"><img src="https://www.sainingxie.com/imgs/portrait.png"   class="speaker_picture" style="width:180px; height:180px;";>
                            <br><br>
                        </a>
                        <p style="font-size:16px; font-weight: 200;">Saining Xie
                            <br>New York University</p>
                </div>
                
                
                    <div class="large-2 columns">
                    <a href="https://yunzhuli.github.io/"><img src="https://yunzhuli.github.io/files/profile.jpg"  class="speaker_picture" style="width:180px; height:180px;">
                        <br><br>
                    </a>
                    <p style="font-size:16px; font-weight: 200;">Yunzhu Li
                        <br>Columbia University</p>
                </div>                            


                <div class="large-2 columns">
                    <a href="https://furong-huang.com/"><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=13yyuCcAAAAJ&citpid=12"  class="speaker_picture" style="width:180px; height:180px;">
                        <br><br>
                    </a>
                    <p style="font-size:16px; font-weight: 200;">Furong Huang
                        <br>University of Maryland</p>
                </div>     
                  
                    
    
                <!-- <div class="large-2 columns">
                    <a href="https://faculty.cc.gatech.edu/~dbatra/"><img src="./static/cvpr2024/img/speakers/dhruv_batra.jpg"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Dhruv Batra
                        <br>Georgia Tech | FAIR</p>
                </div>      -->

                <div class="large-1 columns">
                    <p></p>
                </div>
                    
            </div>

        <hr>
    </div>


    <!-- <div class="row">
        <h2>Schedule (TBD)</h2>
    </div> -->


<br>
<br>
  <!-- <div class="row"> -->
        <!-- <h1 style="font-size:30px; color:grey; font-weight: 200">Invited Speakers</h1> -->
    <h2 style="text-align:center;">Schedule (June 11th, Wednesday)</h2>
    <div id="content-2">
        <div class="schedule">
            <table width="100%">
                <tr>
                    <td width=250>
                        <center><b>9:15 AM - 9:30 AM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="https://jwyang.github.io/images/profile.png"></center>
                    </td>
                    <td style="padding-left:8px"><b>Welcome</b>
                        <!-- [<a href="https://youtu.be/6xNJUiH2kCA">YouTube</a> | <a href="https://www.bilibili.com/video/BV1Ne411w7zx/?share_source=copy_web">Bilibili</a>] -->
                        <a href="https://jwyang.github.io/"><br>Jianwei Yang - Microsoft Research</a>
                        <!-- <br><a target="_blank" href="https://youtu.be/boO0dc1zmQY">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/11mPWEu8fwWMdPeBME3OarcQe2f2TjIxK/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>9:30 AM - 10:05 AM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="./static/cvpr2025/img/speakers/boqing_gong.jpg"></center>
                    </td>
                    <td style="padding-left:8px">
                        <b>Invited Talk</b>

                        <a href="http://boqinggong.info/"><br>Boqing Gong</a>
                        <br>
                        <b>Title: BabyVLM: Democratizing Pretraining of Vision Large Language Models</b>
                        <br>
                        <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Pretraining vision (large) language models (VLLMs) is prohibitively expensive, making it a privilege for institutions with abundant resources and leaving independent researchers to downstream tasks, such as benchmarking, interpreting, and aligning VLLMs. This situation is a crisis for computer vision research --- ‚ÄúWhat I cannot create, I do not understand,‚Äù quoted Richard Feynman. Independent researchers and the public cannot gain a true understanding, trust, and safe use of VLLMs passively from open weights or APIs. Meanwhile, the few privileged VLLM creators could momentarily reach a plateau without the broad research community‚Äôs nurturing. Hence, we propose democratizing VLLM pretraining by scaling it down to a developmentally plausible framework that is scientifically reasonable and computationally friendly to university budgets, aiming to promote exploration rather than exploitation of the pretraining and enable independent researchers to build general-purpose VLLMs that approach ‚Äúbaby intelligence‚Äù to benefit efforts towards ‚Äúgrown-up‚Äù AI. This framework will closely mimic the minimal yet highly informative sensory experiences of human infants, encompassing: 1. Pretraining data curated from longitudinal, egocentric audiovisual recordings of babies. 2. A suite of developmentally aligned evaluation benchmarks assessing VLLM capabilities against cognitive milestones like object permanence, social skills, and language acquisition. 3. A user-friendly pretraining codebase and baseline models.
                        </div>
                        <br>
                        <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Boqing Gong is a computer science faculty member at Boston University and a part-time research scientist at Google DeepMind. His research on machine learning and computer vision focuses on visual recognition, video, and AI models' generalization and efficiency.
                        </div>

                        <!-- <a target="_blank" href="https://youtu.be/SNwSukCtPM0">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/14Vg_zny0Qz2oQjk58WwN6QLBD5sybYd2/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>10:05 AM - 11:00 AM CT</b></center> 
                    </td>
                    <td width=300>
                        <center> 
                            <h1 style="font-size:16px; color:#ffa200; font-weight: 500"> Morning Break
                            <img src="./static/cvpr2025/img/icons/coffee_time.png" class=" style=" width="80" height="100" ;>
                            </h1> 
                            <b></b>
                        </center>
                    </td>
                    <td style="padding-left:8px">
                        <!-- <b>Challenge/Workshop Presentation</b> -->
                        <!-- Poster boards: #135-#142, WEST exhall A
                        <br>-->
                        <!-- <a target="_blank" href="https://youtu.be/dyHlSU7HSN8">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/1QhRQVkqIuGgvtfP61MssHZsM5xXYXs2P/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>11:00 AM - 11:35 AM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=13yyuCcAAAAJ&citpid=12"></center>
                    </td>
                    <td style="padding-left:8px">
                        <b>Invited Talk</b>

                        <a href="https://furong-huang.com/"><br>Furong Huang</a>
                        <br>
                        <b>Title: From Perception to Action: World Model Learning for Generalist Agents¬†</b>
                        <br>
                        <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            This talk explores how to build generalist agents that learn to act by understanding the world‚Äîimplicitly, symbolically, and through exploration. I will present a sequence of frameworks that progressively expand the capabilities of vision-based decision-makers. TACO and Premier-TACO encode temporal structure into the learning objective, implicitly shaping a latent world model that supports few-shot policy learning. FLARE further advances this idea by aligning predictions with future observations to enable long-horizon reasoning. Shifting perspective, TraceVLA introduces visual traces as symbolic prompts that inject structured spatial-temporal priors into generalist policies. Finally, IVE equips agents with the ability to imagine, verify, and execute‚Äîusing vision-language models and memory to explore and collect data autonomously. Together, these works trace a path toward foundation models that integrate perception, reasoning, and control in complex environments. 
                        </div>
                        <br>
                        <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Furong Huang is an Associate Professor of Computer Science at the University of Maryland and a Visiting Research Scholar at Capital One. Her research focuses on trustworthy machine learning, sequential decision-making, and foundation models for perception and control. She has made foundational contributions to world model learning, alignment of generative agents, and robustness in vision-language systems. Dr. Huang‚Äôs recent work explores how agents can implicitly or symbolically construct internal models of the world to support generalization, planning, and exploration. Her research has been recognized with multiple best paper awards and supported by DARPA, NSF, ONR, AFOSR, and industry partners.
                        </div>

                        <!-- <a target="_blank" href="https://youtu.be/SNwSukCtPM0">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/14Vg_zny0Qz2oQjk58WwN6QLBD5sybYd2/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>11:35 AM - 12:05 AM CT</b></center> 
                    </td>
                    <td width=300>
                        <center> 
                            <h1 style="font-size:16px; color:#ffa200; font-weight: 500"> Benchmark Talks
                            <!-- <img src="./static/cvpr2025/img/icons/coffee_time.png" class=" style=" width="80" height="100" ;> -->
                            </h1> 
                            <b></b>
                        </center>
                    </td>
                    <td style="padding-left:8px">
                        <!-- <b>Challenge/Workshop Presentation</b> -->
                        <!-- Poster boards: #135-#142, WEST exhall A
                        <br> -->
                        <!-- <a target="_blank" href="https://youtu.be/dyHlSU7HSN8">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/1QhRQVkqIuGgvtfP61MssHZsM5xXYXs2P/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>1:00 PM - 1:35 PM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="https://www.ranjaykrishna.com/uploads/3/4/4/6/34464733/headshot_orig.jpeg"></center>
                    </td>
                    <td style="padding-left:8px">
                        <b>Invited Talk</b>

                        <a href="https://www.ranjaykrishna.com/index.html"><br>Ranjay Krishna</a>
                        <br>
                        <b>Title: Completely Open Foundation models for Vision, Navigation, & Manipulation</b>
                        <br>
                        <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            What is it going to take to develop completely open-sourced multimodal foundation models? The community's best multimodal language models (GPT, Gemini, Claude, etc) remain proprietary while open-sourced models lag significantly behind. In this talk, I will first introduce Molmo, our completely open multimodal foundation model, which rivaled GPT and outperformed all other models during its release in September 2024. Next, I will delve into open sourced navigation models, trained at scale in simulation. Finally, I will end by describing our work in progress towards developing completely open manipulation VLAs. 
                        </div>
                        <br>
                        <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Ranjay Krishna is an Assistant Professor at the Paul G. Allen School of Computer Science & Engineering. He co-directs the RAIVN lab at UW and leads the computer vision team at Ai2. His research lies at the intersection of computer vision, natural language processing, robotics, and human computer interaction. Ranjay received his PhD degree in Computer Science from Stanford University.
                        </div>

                        <!-- <a target="_blank" href="https://youtu.be/SNwSukCtPM0">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/14Vg_zny0Qz2oQjk58WwN6QLBD5sybYd2/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>1:35 PM - 2:10 PM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="https://yunzhuli.github.io/files/profile.jpg"></center>
                    </td>
                    <td style="padding-left:8px">
                        <b>Invited Talk</b>

                        <a href="https://yunzhuli.github.io/"><br>Yunzhu Li</a>
                        <br>
                        <!-- <b>Title: Completely Open Foundation models for Vision, Navigation, & Manipulation</b>
                        <br>
                        <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            What is it going to take to develop completely open-sourced multimodal foundation models? The community's best multimodal language models (GPT, Gemini, Claude, etc) remain proprietary while open-sourced models lag significantly behind. In this talk, I will first introduce Molmo, our completely open multimodal foundation model, which rivaled GPT and outperformed all other models during its release in September 2024. Next, I will delve into open sourced navigation models, trained at scale in simulation. Finally, I will end by describing our work in progress towards developing completely open manipulation VLAs. 
                        </div>
                        <br> -->
                        <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Yunzhu Li is an Assistant Professor of Computer Science at Columbia University. His research research focuses on advancing robust visual intelligence - the creation of scalable and reliable intelligent systems that can interpret visual events, answer questions about them on demand, and develop a common sense understanding of the world. Yunzhu received his PhD degree in Computer Science from MIT.
                        </div>

                        <!-- <a target="_blank" href="https://youtu.be/SNwSukCtPM0">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/14Vg_zny0Qz2oQjk58WwN6QLBD5sybYd2/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>2:10 PM - 3:00 PM CT</b></center> 
                    </td>
                    <td width=300>
                        <center> 
                            <h1 style="font-size:16px; color:#ffa200; font-weight: 500"> Spotlight Talks
                            <!--  <img src="./static/cvpr2025/img/icons/coffee_time.png" class=" style=" width="80" height="100" ;> -->
                            </h1> 
                            <b></b>
                        </center>
                    </td>
                    <td style="padding-left:8px">
                        <!-- <b>Challenge/Workshop Presentation</b> -->
                        <!-- Poster boards: Room 101 B
                        <br> -->
                        <!-- <a target="_blank" href="https://youtu.be/dyHlSU7HSN8">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/1QhRQVkqIuGgvtfP61MssHZsM5xXYXs2P/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>3:00 PM - 3:35 PM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="https://www.sainingxie.com/imgs/portrait.png"></center>
                    </td>
                    <td style="padding-left:8px">
                        <b>Invited Talk</b>

                        <a href="https://www.sainingxie.com/"><br>Saining Xie</a>
                        <br>
                        <!-- <b>Title: Completely Open Foundation models for Vision, Navigation, & Manipulation</b>
                        <br>
                        <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            What is it going to take to develop completely open-sourced multimodal foundation models? The community's best multimodal language models (GPT, Gemini, Claude, etc) remain proprietary while open-sourced models lag significantly behind. In this talk, I will first introduce Molmo, our completely open multimodal foundation model, which rivaled GPT and outperformed all other models during its release in September 2024. Next, I will delve into open sourced navigation models, trained at scale in simulation. Finally, I will end by describing our work in progress towards developing completely open manipulation VLAs. 
                        </div>
                        <br> -->
                        <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Saining Xie is an Assistant Professor of Computer Science at NYU Courant and part of the CILVR group. He focuses on Robot Learning and aim to significantly expand robots' perception and physical interaction capabilities. Saining received his PhD degree in Computer Science from the University of California, San Diego.
                        </div>

                        <!-- <a target="_blank" href="https://youtu.be/SNwSukCtPM0">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/14Vg_zny0Qz2oQjk58WwN6QLBD5sybYd2/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>3:00 PM - 4:00 PM CT</b></center> 
                    </td>
                    <td width=300>
                        <center> 
                            <h1 style="font-size:16px; color:#ffa200; font-weight: 500"> Coffee Break and Poster Session
                            <img src="./static/cvpr2025/img/icons/coffee_time.png" class=" style=" width="80" height="100" ;>
                            </h1> 
                            <b></b>
                        </center>
                    </td>
                    <td style="padding-left:8px">
                        <!-- <b>Challenge/Workshop Presentation</b> -->
                        Poster boards: Room 101 B (#36-#65)
                        <br>
                        <!-- <a target="_blank" href="https://youtu.be/dyHlSU7HSN8">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/1QhRQVkqIuGgvtfP61MssHZsM5xXYXs2P/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>4:00 PM - 4:35 PM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="https://cordeliaschmid.github.io/images/profil_cd.jpg"></center>
                    </td>
                    <td style="padding-left:8px">
                        <b>Invited Talk</b>

                        <a href="https://cordeliaschmid.github.io/"><br>Cordelia Schmid</a>
                        <br>
                        <b>Title: Video reasoning and grounding: methods & benchmarks</b>
                        <br>
                        <!-- <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            What is it going to take to develop completely open-sourced multimodal foundation models? The community's best multimodal language models (GPT, Gemini, Claude, etc) remain proprietary while open-sourced models lag significantly behind. In this talk, I will first introduce Molmo, our completely open multimodal foundation model, which rivaled GPT and outperformed all other models during its release in September 2024. Next, I will delve into open sourced navigation models, trained at scale in simulation. Finally, I will end by describing our work in progress towards developing completely open manipulation VLAs. 
                        </div> -->
                        <br>
                        <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Cordelia Schmid holds a M.S. degree in Computer Science from the University of Karlsruhe and a Doctorate, also in Computer Science, from the Institut National Polytechnique de Grenoble (INPG). Her doctoral thesis on "Local Greyvalue Invariants for Image Matching and Retrieval" received the best thesis award from INPG in 1996. She received the Habilitation degree in 2001 for her thesis entitled "From Image Matching to Learning Visual Models". Dr. Schmid was a post-doctoral research assistant in the Robotics Research Group of Oxford University in 1996--1997. Since 1997 she has held a permanent research position at Inria, where she is a research director. Dr. Schmid is a member of the German National Academy of Sciences, Leopoldina and a fellow of IEEE and the ELLIS society. She was awarded the Longuet-Higgins prize in 2006, 2014 and 2016, the Koenderink prize in 2018 and the Helmholtz prize in 2023, all for fundamental contributions in computer vision that have withstood the test of time. She received an ERC advanced grant in 2013, the Humboldt research award in 2015, the Inria & French Academy of Science Grand Prix in 2016, the Royal Society Milner award in 2020 and the PAMI distinguished researcher award in 2021. In 2023 she received the K√∂rber European Science Prize and in 2024 the European Inventor Award in the research category. Dr. Schmid has been an Associate Editor for IEEE PAMI (2001--2005) and for IJCV (2004--2012), an editor-in-chief for IJCV (2013--2018), a program chair of IEEE CVPR 2005 and ECCV 2012 as well as a general chair of IEEE CVPR 2015, ECCV 2020 and ICCV 2023. Starting 2018 she holds a joint appointment with Google research.
                        </div>

                        <!-- <a target="_blank" href="https://youtu.be/SNwSukCtPM0">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/14Vg_zny0Qz2oQjk58WwN6QLBD5sybYd2/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

                <tr>
                    <td width=250>
                        <center> <b>4:35 PM - 5:10 PM CT</b></center> 
                    </td>
                    <td width=300>
                        <center><img src="./static/cvpr2025/img/speakers/shizhe_chen.jpg"></center>
                    </td>
                    <td style="padding-left:8px">
                        <b>Invited Talk</b>

                        <a href="https://cshizhe.github.io/"><br>Shizhe Chen</a>
                        <br>
                        <b>Title: Generalization in Vision-Language Guided Robot Manipulation</b>
                        <br>
                        <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Empowering robots to assist in everyday tasks requires a deep understanding of 3D environments, seamless communication with humans, and precise action execution. Yet, existing policies often fall short when faced with novel objects, scenes, or instructions. In this talk, I will share our recent advances in improving robotic perception, reasoning, and acting capabilities. First, I will introduce pretraining 3D vision-language models on synthetic data to enable strong few-shot generalization. Next, I will discuss learning dexterous manipulation skills from human videos. Finally, I will present combining large vision and language models with 3D policies to boost accuracy and generalization.¬† 
                        </div>
                        <br>
                        <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                        <div class="abstract-content">
                            Shizhe Chen is a research scientist at WILLOW project-team in Inria Paris. She received her bachelor‚Äôs and PhD degrees at Renmin University of China in 2015 and 2020 respectively, supervised by Prof. Qin Jin. She then spent two wonderful post-doctoral years at Inria Paris collaborating with Dr. Ivan Laptev and Dr. Cordelia Schmid. Shizhe‚Äôs primary interests lie in embodied AI, vision and language, and multimodal deep learning. She has published over 40 peer-reviewed papers in leading conferences in computer vision, machine learning and robotics such as CVPR, ICCV, ECCV, NeurIPS, ICLR, ACM MM, CoRL, ICRA and IROS. She also served as area chairs in CVPR, ICCV, ECCV, ACM MM, NeurIPS, ICML and ICLR.
                        </div>

                        <!-- <a target="_blank" href="https://youtu.be/SNwSukCtPM0">[Video]</a> -->
                        <!-- <a target="_blank" href="https://drive.google.com/file/d/14Vg_zny0Qz2oQjk58WwN6QLBD5sybYd2/view?usp=sharing">[Slides]</a> -->
                    </td>
                </tr>

            </table>   
        </div>
    </div>

<div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Spotlight Papers</h1>
        <div class="large-12 columns" style="text-align:left;">
            <p style="font-size:15px; font-weight: 400; text-align:left">
                <ul style="font-size:15px;">
                    The following papers are accepted as spotlights to the CVinW Workshop. Congratulations to all authors!
                    <br>
                    <br>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation</a></b></li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</a></b></li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">FLARE: Robot Learning with Implicit World Modeling</a></b></li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">LEMON: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</a></b></li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">PhyWorldBench: A Comprehensive Evaluation of Physical Realism in Text-to-Video Models</a></b> </li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">BLIP3-O: A Family of Fully Open Unified Multimodal Models‚ÄîArchitecture, Training and Dataset</a></b></li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models</a></b></li>
                    <br>  

                    </div>
    
                </ul>    

            </p>
        </div>
    </div>


    <div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Accepted Papers</h1>
        <div class="large-12 columns" style="text-align:left;">
            <p style="font-size:15px; font-weight: 400; text-align:left">
                <ul style="font-size:15px;">
                    The following papers are accepted to the CVinW Workshop. Congratulations to all authors!
                    <br>
                    <br>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning</a></b> Tian Liu, Huixin Zhang, Shubham Parashar, Shu Kong </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream Tasks: Camouflage, Shadow, Medical Image Segmentation, and More</a></b> Tianrun Chen, Ankang Lu, Lanyun Zhu, Chaotao Ding, Chunan Yu, Deyi Ji, Zejian Li, Lingyun Sun, Papa Mao, Ying Zang</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models
</a></b>Pooyan Rahmanzadehgervi, Hung Nguyen, Rosanne Liu, Long Mai, Anh Totti Nguyen</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Improved Convex Decomposition with Ensembling and Boolean Primitives</a></b> Vaibhav Vavilala, Seemandhar Jain, Florian Kluger, Bodo Rosenhahn, David Forsyth, Anand Bhattad</li>

                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation</a></b> Yu Qi, Yuanchen Ju, Tianming Wei, Chi Chu, Lawson L.S. Wong, Huazhe Xu</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations</a></b> Savya Khosla, Sethuraman T V, Alex Schwing, Derek Hoiem</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">CAG‚ÄêVLM : Fine tuning of a large-scale model to recognize angiographic images for next-generation diagnostic systems</a></b> Yuto Nakamura, Satoshi Kodera, Haruki Settai, Hiroki Shinohara, Masatsugu Tamura, Tomohiro Noguchi, Tatsuki Furusawa, Ryo Takizawa, Tempei Kabayama, NORIHIKO TAKEDA</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</a></b> Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">On the Limitations of Vision Language Models in Understanding Image Transforms</a></b> Ahmad Mustafa Anis, M. Saquib Sarfraz, Hasnain Ali Arain</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Deployable Vision AI: Domain-Aware Self-Consistency for Real-World Applications</a></b> Mihir Gupta, Abhay Mangla, Pratik Desai, Ross Greer</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Uncertainty Modeling in Autonomous Vehicle Trajectory Prediction: A Comprehensive Survey</a></b> Siddharth Raina, Jeshwanth Challagundla, Mantek Singh</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">FLARE: Robot Learning with Implicit World Modeling</a></b> Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Lo√Øc Magne, Avnish Narayan, You Liang Tan, Guanzhi Wang, Qi Wang, Jiannan Xiang, Yinzhe Xu, Seonghyeon Ye, Jan Kautz, Furong Huang, Yuke Zhu, Linxi Fan</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Conditioned Image-to-Image Retrieval via Concept-based Visual Projections in Vision-Language Model</a></b> Sohwi Lim, Lee Hyoseok, Tae-Hyun Oh</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Gen4D: Synthesizing Humans and Scenes in the Wild</a></b> Jerrin Bright, Zhibo Wang, Yuhao Chen, Sirisha Rambhatla, David A. Clausi, John S. Zelek</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection</a></b> Shanmukha Vellamcheti, Sanjoy Kundu, Sathyanarayanan N. Aakur</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning</a></b> Xindi Wu, Hee Seung Hwang, Polina Kirichenko, Olga Russakovsky</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">LEMON: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</a></b> Yongyuan Liang, Xiyao Wang, Yuanchen Ju, Jianwei Yang, Furong Huang</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments</a></b> Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Direct Preference Optimization with LLM-as-Judge for Training Computer Use Agents</a></b> Man Luo, David Cobbley, Xin Su, Shachar Rosenman, Vasudev Lal, Shao-Yen Tseng, Phillip Howard</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering</a></b> Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models</a></b> Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">PhyWorldBench: A Comprehensive Evaluation of Physical Realism in Text-to-Video Models</a></b> Jing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui Zhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou, Ming-Yu Liu, Xin Eric Wang</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">Open World Scene Graph Generation using Vision Language Models</a></b> Amartya Dutta, Kazi Sajeed Mehrab, Medha Sawhney, Abhilash Neog, Mridul Khurana, Sepideh Fatemi, Aanish Pradhan, M. Maruf, Ismini Lourentzou, Arka Daw, Anuj Karpatne</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/">BLIP3-O: A Family of Fully Open Unified Multimodal Models‚ÄîArchitecture, Training and Dataset</a></b> Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, silvio savarese, Le Xue, Caiming Xiong, Ran Xu</li>
                    <br>
                    <!-- We accept abstract submissions to our workshop. All submissions shall have maximally 8 pages (excluding references) following the ECCV 2022 author guidelines. All submissions will be reviewed by the Program Committee on the basis of technical quality, relevance to scope of the conference, originality, significance, and clarity. Ther review process is double-blind, and the accepted papers are non-archived in ECCV proceeding. -->
                    <!-- <br> <br> -->
    

                    </div>
    
                </ul>    

            </p>
        </div>
    </div>

                
<div class="row">
    <!-- <h1 style="font-size:30px; color:grey; font-weight: 200">Call for Papers</h1> -->
    <h2>Call for Papers</h2>
    <div class="large-12 columns" style="text-align:left;">
        <p style="font-size:15px; font-weight: 400; text-align:left">
            We welcome original contributions that advance the state of the art in vision-language learning, multimodal perception, and embodied AI, particularly in unconstrained, real-world environments. Topics of interest include, but are not limited to:
            <ul style="font-size:15px;">
                <li style="margin-left:30px">
                    <strong>LMMs & Vision-Language Systems</strong>: Open-vocabulary learning, multimodal pretraining, and adaptation.
                </li>
                <li style="margin-left:30px">
                    <strong>Video Understanding & Temporal Reasoning</strong>: Long-range video modeling, causal reasoning, and instruction-following.
                </li>
                <li style="margin-left:30px">
                    <strong>VLA & Embodied AI</strong>: Multimodal action learning, simulation-to-real transfer, and robotic perception.
                </li>
                <li style="margin-left:30px">
                    <strong>Foundation Models for Vision Tasks</strong>: Object detection, segmentation, tracking, and fine-grained recognition in the wild.
                </li>
                <li style="margin-left:30px">
                    <strong>Efficient Training Methods</strong>: Large visual model adaptation methods, measured by #training samples (zero-shot and few-shot), #trainable parameters, throughput, training cost
                </li>    
                <li style="margin-left:30px">
                    <strong>New Metrics and Benchmarks</strong>: Novel ways to evaluate existing LMMs and large vision models for task-level transfer and open-set visual recognition.
                </li>                            
            </ul>
            <p style="font-size:15px; font-weight: 400; text-align:left">
            We accept abstract submissions to our workshop. All submissions shall have maximally 8 pages (excluding references) following the CVPR 2025 author guidelines. All submissions will be reviewed by the Program Committee on the basis of technical quality, relevance to scope of the conference, originality, significance, and clarity. Ther review process is double-blind, and the accepted papers are <strong>NOT</strong> archived in CVPR 2025 Proceeding.
            </p>          
            <br><br>
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 200; border-style: solid; border-width: 1px; text-align:justify; padding:5px; width:85%">
                    <code>
                        <span style="width:40%; margin:5px; display:inline-block;">Workshop Paper Submission Portal:</span>
                        <span style="display:inline-block; margin-left:10px;"></span><a href="https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/CVinW">[Open Review]</a><img src="./static/cvpr2025/img/icons/icon_live.png" class=" style=" width="45" height="40" ;><br>
                    </code>
                    <code>
                        <!-- <span style="width:40%; margin:5px; display:inline-block;">Feb, 2024</span>
                        <span style="display:inline-block; margin-left:10px;">Competition starts, testing phase begins</span><br> -->

                        <!-- <span style="width:40%; margin:5px; display:inline-block;">June 2nd, 2024</span>
                        <span style="display:inline-block; margin-left:10px;">Competition ends (challenge paper submission)</span><br> -->

                        <span style="width:40%; margin:5px; display:inline-block;">Submission Deadline:</span>
                        <span style="display:inline-block; margin-left:10px;"></span>May 16th, 2025<br>

                        <span style="width:40%; margin:5px; display:inline-block;">Acceptance Notification:</span>
                        <span style="display:inline-block; margin-left:10px;"></span>May 23rd, 2025<br>

                        <span style="width:40%; margin:5px; display:inline-block;">Camera-ready Submission</span>
                        <span style="display:inline-block; margin-left:10px;"></span>May 30th, 2025<br>
                    </code>                    
                </p>
            </div>
            For more information about the paper submission, please reach out the workshop organizers.
        </p>
    </div>
    <hr>
</div>



<div class="row">
            <h2>Call for Challenge Submissions</h2>   
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 400; text-align:left">          
        
        We introduce two new challenges to evaluate the performance of large vision models in the wild:

                <table width="85%">
                <tr>
                    <td width=30>
                        <center> <span style="color:gray;"><b>Challenge</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Task</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Eval Metrics</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Instructions</b></span> <br>
                    </td>                            
                    <td width=170>
                        <center> <span style="color:black;"><b>Make a Challenge Submission</b>
                        </span> <br>
                    </td>                            

                </tr>
                <tr>
                    <td width=30>
                        <center> <span><b><a href="https://temporalbench.github.io/" target="_blank">TemporalBench</a></b></span></center>
                    </td>
                    <td width=150>
                        <center> <span><b>Fine-grained Temporal Video Understanding</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><b>Accuracy</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><a href="https://huggingface.co/datasets/microsoft/TemporalBench"> <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHeW0hoTghUM7U3HDZT7vaVflMW3-dlyZwLw&s" class="style=" width="50" height="50" ;> </a></span></center>
                    </td>     
                    <td width=170>
                        <center> <span><a href="https://temporalbench.github.io/#leaderboard">Leaderboard</a></span></center>
                    </td>                                                        
                </tr>
                <tr>
                    <td width=30>
                        <center> <span><b><a href="https://mmworld-bench.github.io/" target="_blank">MMWorld</a></b></span></center>
                    </td>
                    <td width=150>
                        <center> <span><b>Multi-discipline and multi-facted video understanding</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><b>Accuracy</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><a href="https://github.com/eric-ai-lab/MMWorld"> <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHeW0hoTghUM7U3HDZT7vaVflMW3-dlyZwLw&s" class=" style=" width="50" height="50" ;> </a></span></center>
                    </td>  
                    <td width=170>
                        <center> <span><a href="https://huggingface.co/spaces/Xuehai/MMWorld">Leaderboard</a></span></center>
                    </td>  
                </tr>
                </tr>
            </table>                                 
            <br>
            <br>            
            <div class="large-12 columns" style="text-align:left;">
                <div class="large-12 columns" style="text-align:left;">
                    <p style="font-size:15px; font-weight: 200; border-style: solid; border-width: 1px; text-align:justify; padding:5px; width:85%">
                        <code>    
                            <span style="width:40%; margin:5px; display:inline-block;">June 1st, 2025</span>
                            <span style="display:inline-block; margin-left:10px;">Competition ends</span><br>
    
                            <span style="width:40%; margin:5px; display:inline-block;">June 6th, 2025</span>
                            <span style="display:inline-block; margin-left:10px;">Invitation to present at workshop</span><br>
                        </code>
                    </p>
                </div>
            </div>            
            For more information about the challenge benchmark, please visit the challenge website and reach out the challenge organizers.
            <br><br>
        </p>
    </div>
    <hr> 
</div>



    <!-- <div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Dates</h1>
        <hr>
    </div> -->

<br>

<div class="row">
    <h1 style="font-size:30px; color:black; font-weight: 200">Workshop Organizers</h1>
    <div class="team" id="people">
        <div class="row">


            <div class="large-1 columns">
                <a href="https://jwyang.github.io"><img src="https://jwyang.github.io/images/profile.png" target="_blank" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jianwei Yang
                <br>Microsoft</p>
            </div>

            <div class="large-1 columns">
                <a href="https://chunyuan.li/"><img src="https://vlp-tutorial.github.io/2022/img/organizer/chunyuan.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Chunyuan Li
                    <br>xAI</p>
            </div>
                
            <div class="large-1 columns">
                <a href="https://jiasenlu.github.io/"><img src="https://jiasenlu.github.io/images/profile.jpeg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jiasen Lu
                    <br>Apple</p>
            </div>

            <div class="large-1 columns">
                <a href="https://rxtan2.github.io/"><img src="./static/cvpr2025/img/organizers/reuben.jpg" target="_blank" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Reuben Tan
                    <br>Microsoft
                </p>
            </div>


            <div class="large-1 columns">
                <a href="https://qianhuiwu.github.io/"><img src="https://qianhuiwu.github.io/images/Profile-QianhuiWu.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Qianhui Wu 
                    <br>Microsoft</p>
            </div>             


            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/baolinpeng/"><img src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/avatar-180x180.png" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Baolin Peng
                    <br>Microsoft</p>
            </div>

                    <div class="large-1 columns">
                        <a href="https://www.leizhang.org/"><img src="./static/cvpr2025/img/organizers/leizhang.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Lei Zhang
                    <br>IDEA Research</p>
            </div>

            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/jfgao/"><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=CQ1cqKkAAAAJ&citpid=4" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jianfeng Gao
                    <br>Microsoft</p>
            </div> 
    <hr>
</div>
    </div>



<div class="row">
    <h1 style="font-size:30px; color:black; font-weight: 200">Challenge Organizers</h1>
    <div class="team" id="people">
        <div class="row">

            <div class="large-1 columns">
                <a href="https://pages.cs.wisc.edu/~mucai/"><img src="./static/cvpr2025/img/organizers/mucai_uw1.png" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Mu Cai
                    <br>Google DeepMind</p>
            </div>

            <div class="large-1 columns">
                <a href="https://sheehan1230.github.io/"><img src="https://sheehan1230.github.io/images/profile.png" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Xuehai He
                    <br>UCSC</p>
            </div>

            <div class="large-1 columns">
                    <a href="https://haozhang534.github.io/"><img src="https://haozhang534.github.io/images/photo.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Hao Zhang
                    <br>Nvidia</p>
            </div>

            <div class="large-1 columns">
                <a href="https://rentainhe.github.io/"><img src="./static/cvpr2025/img/organizers/tianhe.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Tianhe Ren
                    <br>IDEA Research</p>
            </div>

            <div class="large-1 columns">
                <a href="https://fengli-ust.github.io/"><img src="https://fengli-ust.github.io/images/myphoto2.jpeg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Feng Li
                    <br>HKUST</p>
            </div>

            <div class="large-1 columns">
                <a href="https://lsl.zone/"><img src="./static/cvpr2025/img/organizers/shilong.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Shilong Liu
                    <br>THU</p>
            </div>

            <div class="large-1 columns">
                <a href="https://maureenzou.github.io/"><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=eslbQqoAAAAJ&citpid=3" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Xueyan Zou
                    <br>UCSD</p>
            </div>

            <div class="large-1 columns">
                <a href="https://zyang-ur.github.io/"><img src="./static/cvpr2025/img/organizers/zhengyuan.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Zhengyuan Yang
                    <br>Microsoft</p>
            </div>

            <div class="large-1 columns">
                <a href="https://eric-xw.github.io/"><img src="https://eric-xw.github.io/pic/xin3.jpeg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Xin (Eric) Wang
                    <br>UCSC</p>
            </div>

            <div class="large-1 columns">
                    <a href="https://pages.cs.wisc.edu/~yongjaelee/?locale=zh-cn"><img src="https://pages.cs.wisc.edu/~yongjaelee/IMAGES/yongjaelee-Nov2019.jpg" class="home_team_picture" style="width:80px; height:80px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yong Jae Lee
                    <br>University of Wisconsin-Madison</p>
            </div>
    <hr>
        </div>
</div>

<div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Program Committee</h1>
    <div class="team" id="people" align=center>
        <div class="row">
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Xueyan Zou (UC San Diego)</p>
            </div>
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Tianhe Ren (IDEA)</p>
            </div>
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Yongyuan Liang (UMD)</p>
            </div>        
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Fangrui Zhu (NEU)</p>
            </div>        
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Jin Gu (UCSC)</p>
            </div>       
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Jiasen Lu (Apple)</p>
            </div>       
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Yanbei Chen (Amazon)</p>
            </div>        
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Sangho Lee (AI2)</p>
            </div>      
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Zhuoran Yu (UW Madison)</p>
            </div>        
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Jianrui Zhang (UW Madison)</p>                
            </div>     
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Jianwei Yang (Microsoft)</p>                
            </div>                
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Feng Li (HKUST)</p>                
            </div>     
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Mu Cai (UW Madison)</p>                
            </div>  
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Xuehai He (USCD)</p>                
            </div>  
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Ruijie Zheng (UMD)</p>                
            </div>     
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Hao Zhang (NVDIA)</p>                
            </div>        
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Zhengyuan Yang (Microsoft)</p>                
            </div>      
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Hanhui Wang (USC)</p>                
            </div>       
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Junyeong Kim (CHUNG-ANG Univ.)</p>                
            </div>   
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Piotr Teterwak (BU)</p>                                
            </div>   
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Nannan Li (BU)</p>                                
            </div>  
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Wenqi Wang (BU)</p>                                
            </div>    
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Mihir Gupta (harker.org)</p>                                
            </div>      
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Jerrin Bright (uwaterloo)</p>                                
            </div>    
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Prasanth Murali (Meta)</p>                                
            </div>   
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Reuben Tan (Microsoft)</p>                                
            </div> 
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Mahir Patel (BU)</p>                                
            </div>      
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Amartya Dutta (VT)</p>                                
            </div>  
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Qianhui Wu (Microsoft)</p>                                
            </div>  
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Yiwu Zhong (CUHK)</p>                                
            </div>  
            <div class="large-2 columns">
                <p style="font-size:14px; font-weight: 200;">Yixin Wan (UCLA)</p>                                
            </div>  
        <hr>
    </div>
</div>

<!-- Challenge Organizers -->



<!-- <div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Challenge Organizers</h1>
    <div class="team" id="people">
        <div class="row">
            <div class="large-1 columns">
                <a href="https://sites.google.com/site/yinfeiyang"><img src="./static/eccv2022/img/yinfei_yang.jpeg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yinfei Yang 
                    <br>Apple</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/yi-ting-chen-google/ "><img src="./static/eccv2022/img/yiting_chen.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yi-Ting Chen 
                    <br>Google</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/ye-xia-19b46b42/ "><img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=QQhJ1pAAAAAJ&citpid=1" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Ye Xia 
                    <br>Google</p>
            </div>

            <div class="large-1 columns">
                <a href="https://openreview.net/profile?id=~Yangguang_Li1 "><img src="./static/eccv2022/img/YangguangLi.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yangguang Li 
                    <br>Sensetime</p>
            </div>
            <div class="large-1 columns">
                    <a href="https://jeff-liangf.github.io"><img src="./static/eccv2022/img/feng_liang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Feng Liang 
                        <br>UT Austin</p>
            </div>
            <div class="large-1 columns">
                <a href="https://slothercui.github.io/ "><img src="./static/eccv2022/img/YufengCui.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yufeng Cui 
                    <br>Sensetime</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/pingjin1"><img src="./static/eccv2022/img/ping_jin.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Ping Jin
                    <br>Microsoft</p>
            </div>   
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/shohei-ono-ab7404a8/"><img src="./static/eccv2022/img/shohei.png" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Shohei Ono 
                    <br>Microsoft</p>
            </div> 
            <div class="large-1 columns">
                <a href="https://houwenpeng.com/"><img src="https://www.microsoft.com/en-us/research/uploads/prod/2019/07/Houwen_Peng_360x360-5d1ce02ab189f.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Houwen Peng
                    <br>Microsoft</p>
            </div> 
            <div class="large-1 columns">
                <a href="https://www.sainingxie.com/"><img src="./static/eccv2022/img/saining.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Saining Xie
                        <br>NYU/Meta</p>
            </div>
            <div class="large-1 columns">
                <a href="https://ancientmooner.github.io/"><img src="./static/eccv2022/img/han_hu.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Han Hu
                        <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
            </div>            
        </div>

        <div class="row">
            <div class="large-1 columns">
                <a href="https://apsdehal.in/"><img src="./static/eccv2022/img/aman.jpeg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Amanpreet Singh
                    <br>HuggingFace</p>
            </div>
            <div class="large-1 columns">
                <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en"><img src="./static/eccv2022/img/xiaojie_jin.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Xiaojie Jin
                        <br>Bytedance</p>
            </div>
            <div class="large-1 columns">
                <a href="https://sites.google.com/site/jshfeng/"><img src="./static/eccv2022/img/jiashi_feng.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Jiashi Feng
                        <br>Bytedance</p>
            </div>
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=qp6IwtgAAAAJ&hl=zh-CN"><img src="./static/eccv2022/img/junyang_lin.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Junyang Lin
                        <br>Alibaba</p>
                </div>
              
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=vO9FZekAAAAJ&hl=zh-CN"><img src="./static/eccv2022/img/an_yang.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">An Yang
                        <br>Alibaba</p>
                </div>

            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=7fjqA0YAAAAJ"><img src="./static/eccv2022/img/peng_wang.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Peng Wang
                        <br>Alibaba</p>
                </div>

            <div class="large-1 columns">
                <a href="https://nguyenbh.github.io/"><img src="https://nguyenbh.github.io/authors/admin/avatar_huaf86694975fe5beecb493d38fc677dd1_22672_270x270_fill_q90_lanczos_center.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Nguyen Bach
                        <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="https://github.com/lynlynlyn"><img src="./static/eccv2022/img/yuning_lu.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Yuning Lu
                        <br>USTC</p>
            </div>    

            <div class="large-1 columns">
                <a href="https://zhangyuanhan-ai.github.io/"><img src="./static/eccv2022/img/yuanhan_zhang.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Yuanhan Zhang
                        <br>NTU</p>
                </div>            

            <div class="large-1 columns">
                <a href="https://kaiyangzhou.github.io/"><img src="https://kaiyangzhou.github.io/images/ky.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Kaiyang Zhou
                        <br>NTU</p>
                </div>

            <div class="large-1 columns">
                <a href="https://liuziwei7.github.io/"><img src="./static/eccv2022/img/ziwei_liu.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Ziwei Liu
                        <br>NTU</p>
            </div>

            <div class="large-1 columns">
            </div>
        </div>

        <div class="row">
            <div class="large-1 columns">
                <a href="https://www.lsl.zone/"><img src="./static/eccv2022/img/shilong_liu.jpeg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Shilong Liu
                    <br>Tsinghua University</p>
            </div>
            <div class="large-1 columns">
                <a href="https://fengli-ust.github.io/"><img src="./static/eccv2022/img/feng_li.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Feng Li
                        <br>HKUST</p>
            </div>
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=zh-CN"><img src="./static/eccv2022/img/hao_zhang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Hao Zhang
                        <br>HKUST</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/jianfengwang1"><img src="./static/eccv2022/img/jianfeng_wang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Jianfeng Wang
                        <br>Microsoft</p>                
            </div>
              
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/lijuanw/"><img src="./static/eccv2022/img/lijuan_wang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Lijuan Wang
                        <br>Microsoft</p>                
            </div>
            
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en"><img src="./static/eccv2022/img/xuehai_he.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Xuehai He
                        <br>UCSC</p>                
            </div>
            
            <div class="large-1 columns">
                <a href="https://eric-xw.github.io/"><img src="./static/eccv2022/img/xin_wang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Xin Eric Wang
                        <br>UCSC</p>                
            </div>

            <div class="large-1 columns">
                <a href="https://cse.buffalo.edu/~changyou/"><img src="./static/eccv2022/img/changyou_chen.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Changyou Chen
                        <br>University at Buffalo, SUNY</p>                   
            </div>

            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/yeahgoyixu"><img src="./static/eccv2022/img/yi_xu.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Yi Xu
                        <br>Amazon</p>                   
            </div>
            
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en"><img src="./static/eccv2022/img/haoxuan_you.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Haoxuan You
                        <br>Columbia University</p>                  
            </div>    


            <div class="large-1 columns">
            </div>            

            <div class="large-1 columns">
            </div>

            <div class="large-1 columns">
            </div>

            <div class="large-1 columns">
            </div>
        </div>

        </div>  

    </div>      


    <hr>    
</div> -->




<!-- <div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Advisory Committee</h1>
    <div class="team" id="people">
        <div class="row">
            <div class="large-1 columns">
                <a href="https://people.eecs.berkeley.edu/~trevor/"><img src="https://www2.eecs.berkeley.edu/Faculty/Photos/Fullsize/darrell.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Trevor Darrell
                    <br>UC Berkley</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.leizhang.org/"><img src="./static/eccv2022/img/leizhang-circle.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Lei Zhang
                    <br>IDEA</p>
            </div>
            <div class="large-1 columns">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/"><img src="./static/eccv2022/img/lee-circle.jpg"" target="_blank" class="home_team_picture style=" width="75" height="75"  ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yong Jae Lee
                    <br>UW Madison</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/houdong-hu-08334227"><img src="./static/eccv2022/img/Houdong_profile.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Houdong Hu
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/zliu/"><img src="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/avatar_user__1490216903-360x360.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Zicheng Liu
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="http://people.csail.mit.edu/celiu/"><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=j7MW4iYAAAAJ&citpid=8" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Ce Liu
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/xdh/"><img src="./static/eccv2022/img/xuedong-circle.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Xuedong Huang
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="http://web.cs.ucla.edu/~kwchang"><img src="./static/eccv2022/img/kaiwei-circle.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Kai-Wei Chang
                    <br>UCLA</p>
            </div>
            <div class="large-1 columns">
                <a href="https://jingdongwang2017.github.io/"><img src="./static/eccv2022/img/WangJingdong.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jingdong Wang
                    <br>Baidu</p>
            </div>  
            <div class="large-1 columns">
                <a href="https://pages.ucsd.edu/~ztu/"><img src="./static/eccv2022/img/ztu_09.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Zhuowen Tu
                    <br>UCSD</p>
            </div>  
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/jfgao/"><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/09/20191101_170350330_iOS-4.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jianfeng Gao
                    <br>Microsoft</p>
            </div>                
            <div class="large-1 columns">
            </div>
        </div>

        <div class="row">
            <div class="large-1 columns">
            </div>
            <div class="large-1 columns">
                <a href="https://people.ece.uw.edu/hwang/"><img src="./static/eccv2022/img/hwang-circle.jpg" target="_blank" class="home_team_picture style=" width="70" height="70" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jenq-Neng Hwang
                    <br>University of Washington</p>
            </div>  
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=WLN3QrAAAAAJ&hl=en"><img src="./static/eccv2022/img/lecun.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yann LeCun
                    <br>NYU/Meta</p>
            </div> 
            <hr>
        </div>        
        <div class="row">
            <div class="disclaimer">
                Disclaimer: To ensure fair comparisons in the challenge, the evaluation server and leaderboards are independently developed and maintained by the <b>Workshop Organizers</b>, while the <b>Challege Organizers</b> actively promote and contribute to the competitions.
            </div>
        </div>
        <hr>
    </div>
</div> -->

        <!-- Contact Information -->
        <div class="large-12 columns" style="background: white;">
            <p style="color:black; text-align:center; display:block; margin-top:7px;">                
                Question? Reach out <a href="https://computer-vision-in-the-wild.github.io/cvpr-2025/" target="_top">Workshop Organizing Team</a>
                <br>
            </p>
        </div>
    </section>
    <script>
    $(document).foundation();
    </script>
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-63638588-1', 'auto');
    ga('send', 'pageview');
    </script>
    <!-- jquery smooth scroll to id's -->
    <script>
    $(function() {
        $('a[href*=#]:not([href=#])').click(function() {
            if (location.pathname.replace(/^\//, '') == this.pathname.replace(/^\//, '') && location.hostname == this.hostname) {
                var target = $(this.hash);
                target = target.length ? target : $('[name=' + this.hash.slice(1) + ']');
                if (target.length) {
                    $('html,body').animate({
                        scrollTop: target.offset().top
                    }, 1000);
                    return false;
                }
            }
        });
    });
    </script>
    <script>
        $(".abstract-toggle").click(function () {

        $header = $(this);
        //getting the next element
        $content = $header.next();
        //open up the content needed - toggle the slide- if visible, slide up, if not slidedown.
        $content.slideToggle(500, function () {
            //execute this after slideToggle is done
            //change text of header based on visibility of content div
            $header.text(function () {
                //change text based on condition
                return $content.is(":visible") ? "[Collapse]" : "[Expand]";
            });
        });

        });
    </script>
</body>

</html>
